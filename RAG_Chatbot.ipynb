{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74041905-c328-4dde-9c73-fc2241a6ca89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.tools.retriever import create_retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c14e4ed4-9dbf-4123-99d7-3814a6909a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_environment():\n",
    "    \"\"\"\n",
    "    Loads environment variables from a .env file and checks for the Groq API key.\n",
    "    \"\"\"\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"ERROR: Groq API key not found.\")\n",
    "        print(\"Please create a .env file and add your GROQ_API_KEY to it.\")\n",
    "        return False\n",
    "    print(\"Groq API Key loaded successfully from .env file.\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae7b2224-f7e8-4cb0-9578-82748602443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_llm():\n",
    "    \"\"\"\n",
    "    Initializes and returns the Groq Language Model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        llm = ChatGroq(model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\", temperature=0)\n",
    "        print(\"Groq LLM (Llama3-8b) initialized.\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Groq LLM: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b1f3422-d3c7-4715-8aff-a23e192d4ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_documents(url):\n",
    "    \"\"\"\n",
    "    Loads documents from a URL and splits them into manageable chunks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loader = WebBaseLoader(url)\n",
    "        docs = loader.load()\n",
    "        print(f\"Loaded {len(docs)} document(s) from the web.\")\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        splits = text_splitter.split_documents(docs)\n",
    "        print(f\"Document split into {len(splits)} chunks.\")\n",
    "        return splits\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or splitting documents: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47911613-0723-452f-81e1-897b484d6462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(splits):\n",
    "    \"\"\"\n",
    "    Creates a ChromaDB vector store from document splits.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Initializing embedding model and vector store (ChromaDB)...\")\n",
    "        embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)\n",
    "        print(\"Vector store created successfully.\")\n",
    "        return vectorstore\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector store: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c45bc1d-9a2c-4be2-a27f-3cadf3d45d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent_executor(llm, vectorstore):\n",
    "    \"\"\"\n",
    "    Creates and returns a LangChain agent executor.\n",
    "    This agent is equipped with a retriever tool to perform RAG.\n",
    "    \"\"\"\n",
    "    print(\"Creating RAG tool and agent...\")\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    # Create a tool that uses our retriever\n",
    "    retriever_tool = create_retriever_tool(\n",
    "        retriever,\n",
    "        \"document_search\",\n",
    "        \"Search for information from the provided document. For any questions about the document, you must use this tool!\",\n",
    "    )\n",
    "\n",
    "    # Define the set of tools the agent can use\n",
    "    tools = [retriever_tool]\n",
    "\n",
    "    # Create the agent prompt\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant. You have access to a tool for searching \"\n",
    "        \"information within a document provided by the user.\"\n",
    "    )\n",
    "    agent_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\"),\n",
    "            (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create the agent itself\n",
    "    agent = create_tool_calling_agent(llm, tools, agent_prompt)\n",
    "\n",
    "    # Create the Agent Executor, which runs the agent and its tools\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "    print(\"Agent with RAG tool is ready.\")\n",
    "    return agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "609ba4ef-523d-471b-a129-0e1ff3439994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_interactive_session(agent_executor):\n",
    "    \"\"\"\n",
    "    Starts the interactive Q&A session with the user.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- RAG Q&A System is Running ---\")\n",
    "    print(\"You can now ask questions about the content of the provided web page.\")\n",
    "    print(\"Type 'exit' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nYour Question: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Exiting the system. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # The agent decides whether to use the retriever tool for relevant questions.\n",
    "            response = agent_executor.invoke({\"input\": user_input})\n",
    "            print(\"\\nAnswer:\", response['output'])\n",
    "        except Exception as e:\n",
    "            print(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f80ba3b-9ef5-4ec4-8ce8-d312ad766fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the setup and execution of the RAG Q&A system.\n",
    "    \"\"\"\n",
    "    if not setup_environment():\n",
    "        return\n",
    "\n",
    "    llm = initialize_llm()\n",
    "    if not llm:\n",
    "        return\n",
    "\n",
    "    # Get the document URL from the user\n",
    "    document_url = input(\"Please enter the URL of the document you want to query: \")\n",
    "    if not document_url:\n",
    "        print(\"No URL provided. Exiting.\")\n",
    "        return\n",
    "\n",
    "    doc_splits = load_and_split_documents(document_url)\n",
    "    if not doc_splits:\n",
    "        return\n",
    "\n",
    "    vector_store = create_vector_store(doc_splits)\n",
    "    if not vector_store:\n",
    "        return\n",
    "\n",
    "    agent_executor = create_agent_executor(llm, vector_store)\n",
    "    if not agent_executor:\n",
    "        return\n",
    "\n",
    "    start_interactive_session(agent_executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0635f4-516c-4a27-8917-c8a700963bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq API Key loaded successfully from .env file.\n",
      "Groq LLM (Llama3-8b) initialized.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter the URL of the document you want to query:  https://developers.google.com/machine-learning/resources/intro-llms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s) from the web.\n",
      "Document split into 15 chunks.\n",
      "Initializing embedding model and vector store (ChromaDB)...\n",
      "Vector store created successfully.\n",
      "Creating RAG tool and agent...\n",
      "Agent with RAG tool is ready.\n",
      "\n",
      "--- RAG Q&A System is Running ---\n",
      "You can now ask questions about the content of the provided web page.\n",
      "Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your Question:  What is LLM?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLLM stands for Large Language Model. It refers to a type of artificial intelligence (AI) model designed to process and understand human language at a large scale. These models are trained on vast amounts of text data, which enables them to learn patterns, relationships, and context within language. This training allows LLMs to generate text, answer questions, translate languages, and perform various other natural language processing tasks. They are a significant advancement in the field of natural language processing (NLP) and have numerous applications in areas such as customer service, language translation, content generation, and more.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Answer: LLM stands for Large Language Model. It refers to a type of artificial intelligence (AI) model designed to process and understand human language at a large scale. These models are trained on vast amounts of text data, which enables them to learn patterns, relationships, and context within language. This training allows LLMs to generate text, answer questions, translate languages, and perform various other natural language processing tasks. They are a significant advancement in the field of natural language processing (NLP) and have numerous applications in areas such as customer service, language translation, content generation, and more.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your Question:  How large is LLM?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `document_search` with `{'query': 'LLM size'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m(4) Response generation: LLM receives the execution results and provides summarized results to users.\n",
      "To put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\n",
      "\n",
      "Overview of a LLM-powered autonomous agent system.\n",
      "\n",
      "API-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\u001b[0m\u001b[32;1m\u001b[1;3mThe size of LLM (Large Language Model) is not explicitly stated in the search results. However, I can tell you that LLMs can vary greatly in size, typically measured in terms of the number of parameters, which can range from hundreds of millions to tens of billions. \n",
      "\n",
      "If you need more specific information about the size of a particular LLM, please provide more context or details about the model you are referring to.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Answer: The size of LLM (Large Language Model) is not explicitly stated in the search results. However, I can tell you that LLMs can vary greatly in size, typically measured in terms of the number of parameters, which can range from hundreds of millions to tens of billions. \n",
      "\n",
      "If you need more specific information about the size of a particular LLM, please provide more context or details about the model you are referring to.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your Question:  What is Transformers?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTransformers is a type of neural network architecture introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017. It's primarily used for natural language processing tasks, such as machine translation, text classification, and language generation.\n",
      "\n",
      "The Transformer model relies on self-attention mechanisms to weigh the importance of different input elements relative to each other, allowing it to capture long-range dependencies and contextual relationships in sequential data.\n",
      "\n",
      "Transformers have become a foundational component in many state-of-the-art language models, including BERT, RoBERTa, and XLNet, and have achieved remarkable results in various NLP tasks.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Answer: Transformers is a type of neural network architecture introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017. It's primarily used for natural language processing tasks, such as machine translation, text classification, and language generation.\n",
      "\n",
      "The Transformer model relies on self-attention mechanisms to weigh the importance of different input elements relative to each other, allowing it to capture long-range dependencies and contextual relationships in sequential data.\n",
      "\n",
      "Transformers have become a foundational component in many state-of-the-art language models, including BERT, RoBERTa, and XLNet, and have achieved remarkable results in various NLP tasks.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
